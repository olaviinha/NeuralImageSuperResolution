{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LatentDiffusionFBCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ijcNgzL7VZgfqHYldITYKvOrdId3lUo-",
      "authorship_tag": "ABX9TyP8HtHG+S4JET+XDo3emw6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralImageSuperResolution/blob/master/LatentDiffusionFBCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCskRiNym_LW"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Neural Image Super-Resolution<font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font color=\"#999\" size=\"4\">Latent Diffusion + FBCNN</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralImageSuperResolution\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a>\n",
        "\n",
        "This notebook combines [Latent Diffusion](https://github.com/CompVis/latent-diffusion) and [FBCNN](https://github.com/jiaxi-jiang/FBCNN) in an attempt to improve and enhance image quality.\n",
        "\n",
        "This notebook does not increase image resolution.\n",
        "\n",
        "`local_models_dir` and `input_img` should be relative to your Google Drive root. I.e. if your Google Drive has a directory called _images_ and under that directory you have a file _face.jpg_, then `input_img` value should be `images/face.jpg`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6-8KXU6m8TM",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.<br>\n",
        "#@markdown <small>- Mounting Drive will enable this notebook to save outputs directly to your Drive. Otherwise you will need to copy/download them manually from this notebook.</small>\n",
        "#@markdown <small><br>- `local_models_dir` is a directory path in your Google Drive to store models used by this notebook. Useful in case you don't want the notebook to download the models every time or in case the third party model download links get broken in the future.</small>\n",
        "\n",
        "force_setup = False\n",
        "pip_packages = ''\n",
        "main_repository = 'https://github.com/olaviinha/FBCNN.git'\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "local_models_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Download the repo from Github\n",
        "import os\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%cd /content/\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if mount_drive is True:\n",
        "  if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    drive_root = '/content/drive/My Drive'\n",
        "  if not os.path.isdir('/content/mydrive'):\n",
        "    os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "    drive_root = '/content/mydrive/'\n",
        "  drive_root_set = True\n",
        "else:\n",
        "  create_dirs(['/content/faux_drive'])\n",
        "  drive_root = '/content/faux_drive/'\n",
        "\n",
        "if main_repository is not '':\n",
        "  !git clone {main_repository}\n",
        "\n",
        "if local_models_dir is '':\n",
        "  local_models_dir = '/content/FBCNN/models/'\n",
        "  r = requests.get('https://github.com/jiaxi-jiang/FBCNN/releases/download/v1.0/fbcnn_color.pth', allow_redirects=True)\n",
        "  open(local_models_dir, 'wb').write(r.content)\n",
        "  r = requests.get('https://github.com/jiaxi-jiang/FBCNN/releases/download/v1.0/fbcnn_gray.pth', allow_redirects=True)\n",
        "  open(local_models_dir, 'wb').write(r.content)\n",
        "else:\n",
        "  local_models_dir = drive_root+fix_path(local_models_dir)\n",
        "\n",
        "# JPEG artifact removal\n",
        "sys.path.insert(1, '/content/FBCNN/models')\n",
        "sys.path.insert(1, '/content/FBCNN/utils')\n",
        "import os.path\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "import utils_image as util\n",
        "from network_fbcnn import FBCNN as net\n",
        "import requests\n",
        "import datetime\n",
        "\n",
        "# SuperRes\n",
        "!git clone https://github.com/CompVis/latent-diffusion.git\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install -e ./taming-transformers\n",
        "!pip install ipywidgets omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops wandb\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "sys.path.append(\".\")\n",
        "sys.path.append('./taming-transformers')\n",
        "from taming.models import vqgan # checking correct import from taming\n",
        "from torchvision.datasets.utils import download_url\n",
        "%cd '/content/latent-diffusion'\n",
        "from functools import partial\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like\n",
        "# from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.util import ismap\n",
        "%cd '/content'\n",
        "from google.colab import files\n",
        "from IPython.display import Image as ipyimg\n",
        "from numpy import asarray\n",
        "from einops import rearrange, repeat\n",
        "import torch, torchvision\n",
        "import time\n",
        "from omegaconf import OmegaConf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def sr_run(model, img, task, custom_steps, eta, resize_enabled=False, classifier_ckpt=None, global_step=None):\n",
        "    # global stride\n",
        "\n",
        "    example = get_cond(task, img)\n",
        "\n",
        "    save_intermediate_vid = False\n",
        "    n_runs = 1\n",
        "    masked = False\n",
        "    guider = None\n",
        "    ckwargs = None\n",
        "    mode = 'ddim'\n",
        "    ddim_use_x0_pred = False\n",
        "    temperature = 1.\n",
        "    eta = eta\n",
        "    make_progrow = True\n",
        "    custom_shape = None\n",
        "\n",
        "    height, width = example[\"image\"].shape[1:3]\n",
        "    split_input = height >= 128 and width >= 128\n",
        "\n",
        "    if split_input:\n",
        "        ks = 128\n",
        "        stride = 64\n",
        "        vqf = 4  #\n",
        "        model.split_input_params = {\"ks\": (ks, ks), \"stride\": (stride, stride),\n",
        "                                    \"vqf\": vqf,\n",
        "                                    \"patch_distributed_vq\": True,\n",
        "                                    \"tie_braker\": False,\n",
        "                                    \"clip_max_weight\": 0.5,\n",
        "                                    \"clip_min_weight\": 0.01,\n",
        "                                    \"clip_max_tie_weight\": 0.5,\n",
        "                                    \"clip_min_tie_weight\": 0.01}\n",
        "    else:\n",
        "        if hasattr(model, \"split_input_params\"):\n",
        "            delattr(model, \"split_input_params\")\n",
        "\n",
        "    invert_mask = False\n",
        "\n",
        "    x_T = None\n",
        "    for n in range(n_runs):\n",
        "        if custom_shape is not None:\n",
        "            x_T = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n",
        "            x_T = repeat(x_T, '1 c h w -> b c h w', b=custom_shape[0])\n",
        "\n",
        "        logs = make_convolutional_sample(example, model,\n",
        "                                         mode=mode, custom_steps=custom_steps,\n",
        "                                         eta=eta, swap_mode=False , masked=masked,\n",
        "                                         invert_mask=invert_mask, quantize_x0=False,\n",
        "                                         custom_schedule=None, decode_interval=10,\n",
        "                                         resize_enabled=resize_enabled, custom_shape=custom_shape,\n",
        "                                         temperature=temperature, noise_dropout=0.,\n",
        "                                         corrector=guider, corrector_kwargs=ckwargs, x_T=x_T, save_intermediate_vid=save_intermediate_vid,\n",
        "                                         make_progrow=make_progrow,ddim_use_x0_pred=ddim_use_x0_pred\n",
        "                                         )\n",
        "    return logs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def do_superres(img, mode):\n",
        "\n",
        "  if mode == 'Faster':\n",
        "      sr_diffusion_steps = \"25\" \n",
        "      sr_pre_downsample = '1/2' \n",
        "  if mode == 'Fast':\n",
        "      sr_diffusion_steps = \"100\" \n",
        "      sr_pre_downsample = '1/2' \n",
        "  if mode == 'Slow':\n",
        "      sr_diffusion_steps = \"25\" \n",
        "      sr_pre_downsample = 'None' \n",
        "  if mode == 'Very Slow':\n",
        "      sr_diffusion_steps = \"100\" \n",
        "      sr_pre_downsample = 'None' \n",
        "\n",
        "  # sr_diffusion_steps = \"100\" \n",
        "  # sr_pre_downsample = 'None' \n",
        "\n",
        "  sr_post_downsample = 'Original Size'\n",
        "  sr_diffusion_steps = int(sr_diffusion_steps)\n",
        "  sr_eta = 1.0 \n",
        "  sr_downsample_method = 'Lanczos' \n",
        "\n",
        "  im_og = img\n",
        "  width_og, height_og = im_og.size\n",
        "\n",
        "  #Downsample Pre\n",
        "  if sr_pre_downsample == '1/2':\n",
        "    downsample_rate = 2\n",
        "  elif sr_pre_downsample == '1/4':\n",
        "    downsample_rate = 4\n",
        "  else:\n",
        "    downsample_rate = 1\n",
        "\n",
        "  width_downsampled_pre = width_og//downsample_rate\n",
        "  height_downsampled_pre = height_og//downsample_rate\n",
        "\n",
        "  if downsample_rate != 1:\n",
        "    # print(f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n",
        "    im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n",
        "    # im_og.save('/content/temp.png')\n",
        "    # filepath = '/content/temp.png'\n",
        "\n",
        "  logs = sr_run(sr_model[\"model\"], im_og, sr_diffMode, sr_diffusion_steps, sr_eta)\n",
        "\n",
        "  sample = logs[\"sample\"]\n",
        "  sample = sample.detach().cpu()\n",
        "  sample = torch.clamp(sample, -1., 1.)\n",
        "  sample = (sample + 1.) / 2. * 255\n",
        "  sample = sample.numpy().astype(np.uint8)\n",
        "  sample = np.transpose(sample, (0, 2, 3, 1))\n",
        "  a = Image.fromarray(sample[0])\n",
        "\n",
        "  #Downsample Post\n",
        "  if sr_post_downsample == '1/2':\n",
        "    downsample_rate = 2\n",
        "  elif sr_post_downsample == '1/4':\n",
        "    downsample_rate = 4\n",
        "  else:\n",
        "    downsample_rate = 1\n",
        "\n",
        "  width, height = a.size\n",
        "  width_downsampled_post = width//downsample_rate\n",
        "  height_downsampled_post = height//downsample_rate\n",
        "\n",
        "  if sr_downsample_method == 'Lanczos':\n",
        "    aliasing = Image.LANCZOS\n",
        "  else:\n",
        "    aliasing = Image.NEAREST\n",
        "\n",
        "  if downsample_rate != 1:\n",
        "    a = a.resize((width_downsampled_post, height_downsampled_post), aliasing)\n",
        "  elif sr_post_downsample == 'Original Size':\n",
        "    a = a.resize((width_og, height_og), aliasing)\n",
        "\n",
        "  # display.display(a)\n",
        "  # a.save(filepath)\n",
        "  return a\n",
        "\n",
        "  \n",
        "def remove_artifacts(input_img, input_quality):\n",
        "  input_img_width, input_img_height = input_img.size\n",
        "  if is_gray:\n",
        "    n_channels = 1 # set 1 for grayscale image, set 3 for color image\n",
        "    model_name = 'fbcnn_gray.pth'\n",
        "  else:\n",
        "    n_channels = 3 # set 1 for grayscale image, set 3 for color image\n",
        "    model_name = 'fbcnn_color.pth'\n",
        "  nc = [64,128,256,512]\n",
        "  nb = 4\n",
        "  input_quality = 100 - input_quality\n",
        "  model_path = local_models_dir+model_name\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = net(in_nc=n_channels, out_nc=n_channels, nc=nc, nb=nb, act_mode='R')\n",
        "  model.load_state_dict(torch.load(model_path), strict=True)\n",
        "  model.eval()\n",
        "  for k, v in model.named_parameters():\n",
        "      v.requires_grad = False\n",
        "  model = model.to(device)\n",
        "  test_results = OrderedDict()\n",
        "  test_results['psnr'] = []\n",
        "  test_results['ssim'] = []\n",
        "  test_results['psnrb'] = []\n",
        "  if n_channels == 1:\n",
        "      open_cv_image = Image.fromarray(input_img)\n",
        "      # open_cv_image = input_img\n",
        "      open_cv_image = ImageOps.grayscale(open_cv_image)\n",
        "      open_cv_image = np.array(open_cv_image) # PIL to open cv image\n",
        "      img = np.expand_dims(open_cv_image, axis=2)  # HxWx1\n",
        "  elif n_channels == 3:\n",
        "      open_cv_image = np.array(input_img) # PIL to open cv image\n",
        "      # open_cv_image = input_img # PIL to open cv image\n",
        "      if open_cv_image.ndim == 2:\n",
        "          open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_GRAY2RGB)  # GGG\n",
        "      else:\n",
        "          open_cv_image = cv2.cvtColor(open_cv_image, cv2.COLOR_BGR2RGB)  # RGB\n",
        "  img_L = util.uint2tensor4(open_cv_image)\n",
        "  img_L = img_L.to(device)\n",
        "  img_E,QF = model(img_L)\n",
        "  img_E = util.tensor2single(img_E)\n",
        "  img_E = util.single2uint(img_E)\n",
        "  qf_input = torch.tensor([[1-input_quality/100]]).cuda() if device == torch.device('cuda') else torch.tensor([[1-input_quality/100]])\n",
        "  img_E,QF = model(img_L, qf_input)  \n",
        "  img_E = util.tensor2single(img_E)\n",
        "  img_E = util.single2uint(img_E)\n",
        "\n",
        "  if img_E.ndim == 3:\n",
        "    img_E = img_E[:, :, [2, 1, 0]]\n",
        "  out_img = Image.fromarray(img_E)\n",
        "\n",
        "  return out_img\n",
        "\n",
        "\n",
        "def download_models(mode):\n",
        "  if mode == \"superresolution\":\n",
        "    # this is the small bsr light model\n",
        "    url_conf = 'https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1'\n",
        "    url_ckpt = 'https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1'\n",
        "\n",
        "    path_conf = f'{local_models_dir}/superres/project.yaml'\n",
        "    path_ckpt = f'{local_models_dir}/superres/last.ckpt'\n",
        "\n",
        "    download_url(url_conf, path_conf)\n",
        "    download_url(url_ckpt, path_ckpt)\n",
        "\n",
        "    path_conf = path_conf + '/?dl=1' # fix it\n",
        "    path_ckpt = path_ckpt + '/?dl=1' # fix it\n",
        "    return path_conf, path_ckpt\n",
        "\n",
        "  else:\n",
        "    raise NotImplementedError\n",
        "      \n",
        "def get_model(mode):\n",
        "  path_conf, path_ckpt = download_models(mode)\n",
        "  config = OmegaConf.load(path_conf)\n",
        "  model, step = load_model_from_config(config, path_ckpt)\n",
        "  return model\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "  #print(f\"Loading model from {ckpt}\")\n",
        "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "  global_step = pl_sd[\"global_step\"]\n",
        "  sd = pl_sd[\"state_dict\"]\n",
        "  model = instantiate_from_config(config.model)\n",
        "  m, u = model.load_state_dict(sd, strict=False)\n",
        "  model.cuda()\n",
        "  model.eval()\n",
        "  return {\"model\": model}, global_step\n",
        "\n",
        "\n",
        "def get_cond(mode, img):\n",
        "  example = dict()\n",
        "  if mode == \"superresolution\":\n",
        "    up_f = 4\n",
        "    c = img\n",
        "    c = torch.unsqueeze(torchvision.transforms.ToTensor()(c), 0)\n",
        "    c_up = torchvision.transforms.functional.resize(c, size=[up_f * c.shape[2], up_f * c.shape[3]], antialias=True)\n",
        "    c_up = rearrange(c_up, '1 c h w -> 1 h w c')\n",
        "    c = rearrange(c, '1 c h w -> 1 h w c')\n",
        "    c = 2. * c - 1.\n",
        "    c = c.to(torch.device(\"cuda\"))\n",
        "    example[\"LR_image\"] = c\n",
        "    example[\"image\"] = c_up\n",
        "  return example\n",
        "\n",
        "class DDIMSampler(object):\n",
        "    def __init__(self, model, schedule=\"linear\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.ddpm_num_timesteps = model.num_timesteps\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def register_buffer(self, name, attr):\n",
        "        if type(attr) == torch.Tensor:\n",
        "            if attr.device != torch.device(\"cuda\"):\n",
        "                attr = attr.to(torch.device(\"cuda\"))\n",
        "        setattr(self, name, attr)\n",
        "\n",
        "    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n",
        "        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n",
        "                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n",
        "        alphas_cumprod = self.model.alphas_cumprod\n",
        "        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n",
        "        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n",
        "\n",
        "        self.register_buffer('betas', to_torch(self.model.betas))\n",
        "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
        "        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n",
        "\n",
        "        # ddim sampling parameters\n",
        "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
        "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
        "                                                                                   eta=ddim_eta,verbose=verbose)\n",
        "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
        "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
        "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
        "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
        "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n",
        "            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n",
        "                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
        "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self,\n",
        "               S,\n",
        "               batch_size,\n",
        "               shape,\n",
        "               conditioning=None,\n",
        "               callback=None,\n",
        "               normals_sequence=None,\n",
        "               img_callback=None,\n",
        "               quantize_x0=False,\n",
        "               eta=0.,\n",
        "               mask=None,\n",
        "               x0=None,\n",
        "               temperature=1.,\n",
        "               noise_dropout=0.,\n",
        "               score_corrector=None,\n",
        "               corrector_kwargs=None,\n",
        "               verbose=True,\n",
        "               x_T=None,\n",
        "               log_every_t=100,\n",
        "               **kwargs\n",
        "               ):\n",
        "        if conditioning is not None:\n",
        "            if isinstance(conditioning, dict):\n",
        "                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n",
        "                if cbs != batch_size:\n",
        "                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n",
        "            else:\n",
        "                if conditioning.shape[0] != batch_size:\n",
        "                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n",
        "\n",
        "        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n",
        "        # sampling\n",
        "        C, H, W = shape\n",
        "        size = (batch_size, C, H, W)\n",
        "        # print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n",
        "\n",
        "        samples, intermediates = self.ddim_sampling(conditioning, size,\n",
        "                                                    callback=callback,\n",
        "                                                    img_callback=img_callback,\n",
        "                                                    quantize_denoised=quantize_x0,\n",
        "                                                    mask=mask, x0=x0,\n",
        "                                                    ddim_use_original_steps=False,\n",
        "                                                    noise_dropout=noise_dropout,\n",
        "                                                    temperature=temperature,\n",
        "                                                    score_corrector=score_corrector,\n",
        "                                                    corrector_kwargs=corrector_kwargs,\n",
        "                                                    x_T=x_T,\n",
        "                                                    log_every_t=log_every_t\n",
        "                                                    )\n",
        "        return samples, intermediates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ddim_sampling(self, cond, shape,\n",
        "                      x_T=None, ddim_use_original_steps=False,\n",
        "                      callback=None, timesteps=None, quantize_denoised=False,\n",
        "                      mask=None, x0=None, img_callback=None, log_every_t=100,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n",
        "        device = self.model.betas.device\n",
        "        b = shape[0]\n",
        "        if x_T is None:\n",
        "            img = torch.randn(shape, device=device)\n",
        "        else:\n",
        "            img = x_T\n",
        "\n",
        "        if timesteps is None:\n",
        "            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n",
        "        elif timesteps is not None and not ddim_use_original_steps:\n",
        "            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n",
        "            timesteps = self.ddim_timesteps[:subset_end]\n",
        "\n",
        "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
        "        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n",
        "        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n",
        "        #print(f\"Running DDIM Sharpening with {total_steps} timesteps\")\n",
        "\n",
        "        iterator = tqdm(time_range, desc='DDIM Sharpening', total=total_steps)\n",
        "\n",
        "        for i, step in enumerate(iterator):\n",
        "            index = total_steps - i - 1\n",
        "            ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
        "\n",
        "            if mask is not None:\n",
        "                assert x0 is not None\n",
        "                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n",
        "                img = img_orig * mask + (1. - mask) * img\n",
        "\n",
        "            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
        "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
        "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
        "                                      corrector_kwargs=corrector_kwargs)\n",
        "            img, pred_x0 = outs\n",
        "            if callback: callback(i)\n",
        "            if img_callback: img_callback(pred_x0, i)\n",
        "\n",
        "            if index % log_every_t == 0 or index == total_steps - 1:\n",
        "                intermediates['x_inter'].append(img)\n",
        "                intermediates['pred_x0'].append(pred_x0)\n",
        "\n",
        "        return img, intermediates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "        e_t = self.model.apply_model(x, t, c)\n",
        "        if score_corrector is not None:\n",
        "            assert self.model.parameterization == \"eps\"\n",
        "            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n",
        "\n",
        "        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n",
        "        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n",
        "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n",
        "        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n",
        "        # select parameters corresponding to the currently considered timestep\n",
        "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
        "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
        "        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
        "        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
        "\n",
        "        # current prediction for x_0\n",
        "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
        "        if quantize_denoised:\n",
        "            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n",
        "        # direction pointing to x_t\n",
        "        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
        "        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
        "        if noise_dropout > 0.:\n",
        "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
        "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
        "        return x_prev, pred_x0\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def make_convolutional_sample(batch, model, mode=\"vanilla\", custom_steps=None, eta=1.0, swap_mode=False, masked=False,\n",
        "                              invert_mask=True, quantize_x0=False, custom_schedule=None, decode_interval=1000,\n",
        "                              resize_enabled=False, custom_shape=None, temperature=1., noise_dropout=0., corrector=None,\n",
        "                              corrector_kwargs=None, x_T=None, save_intermediate_vid=False, make_progrow=True,ddim_use_x0_pred=False):\n",
        "    log = dict()\n",
        "    z, c, x, xrec, xc = model.get_input(batch, model.first_stage_key,\n",
        "                                        return_first_stage_outputs=True,\n",
        "                                        force_c_encode=not (hasattr(model, 'split_input_params')\n",
        "                                                            and model.cond_stage_key == 'coordinates_bbox'),\n",
        "                                        return_original_cond=True)\n",
        "\n",
        "    log_every_t = 1 if save_intermediate_vid else None\n",
        "    if custom_shape is not None:\n",
        "        z = torch.randn(custom_shape)\n",
        "        # print(f\"Generating {custom_shape[0]} samples of shape {custom_shape[1:]}\")\n",
        "    z0 = None\n",
        "    log[\"input\"] = x\n",
        "    log[\"reconstruction\"] = xrec\n",
        "    if ismap(xc):\n",
        "        log[\"original_conditioning\"] = model.to_rgb(xc)\n",
        "        if hasattr(model, 'cond_stage_key'):\n",
        "            log[model.cond_stage_key] = model.to_rgb(xc)\n",
        "    else:\n",
        "        log[\"original_conditioning\"] = xc if xc is not None else torch.zeros_like(x)\n",
        "        if model.cond_stage_model:\n",
        "            log[model.cond_stage_key] = xc if xc is not None else torch.zeros_like(x)\n",
        "            if model.cond_stage_key =='class_label':\n",
        "                log[model.cond_stage_key] = xc[model.cond_stage_key]\n",
        "    with model.ema_scope(\"Plotting\"):\n",
        "        t0 = time.time()\n",
        "        img_cb = None\n",
        "        sample, intermediates = convsample_ddim(model, c, steps=custom_steps, shape=z.shape,\n",
        "                                                eta=eta,\n",
        "                                                quantize_x0=quantize_x0, img_callback=img_cb, mask=None, x0=z0,\n",
        "                                                temperature=temperature, noise_dropout=noise_dropout,\n",
        "                                                score_corrector=corrector, corrector_kwargs=corrector_kwargs,\n",
        "                                                x_T=x_T, log_every_t=log_every_t)\n",
        "        t1 = time.time()\n",
        "        if ddim_use_x0_pred:\n",
        "            sample = intermediates['pred_x0'][-1]\n",
        "    x_sample = model.decode_first_stage(sample)\n",
        "    try:\n",
        "        x_sample_noquant = model.decode_first_stage(sample, force_not_quantize=True)\n",
        "        log[\"sample_noquant\"] = x_sample_noquant\n",
        "        log[\"sample_diff\"] = torch.abs(x_sample_noquant - x_sample)\n",
        "    except:\n",
        "        pass\n",
        "    log[\"sample\"] = x_sample\n",
        "    log[\"time\"] = t1 - t0\n",
        "    return log\n",
        "\n",
        "@torch.no_grad()\n",
        "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n",
        "                    mask=None, x0=None, quantize_x0=False, img_callback=None,\n",
        "                    temperature=1., noise_dropout=0., score_corrector=None,\n",
        "                    corrector_kwargs=None, x_T=None, log_every_t=None\n",
        "                    ):\n",
        "    ddim = DDIMSampler(model)\n",
        "    bs = shape[0]  # dont know where this comes from but wayne\n",
        "    shape = shape[1:]  # cut batch dim\n",
        "    samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape, conditioning=cond, callback=callback,\n",
        "                                         normals_sequence=normals_sequence, quantize_x0=quantize_x0, eta=eta,\n",
        "                                         mask=mask, x0=x0, temperature=temperature, verbose=False,\n",
        "                                         score_corrector=score_corrector,\n",
        "                                         corrector_kwargs=corrector_kwargs, x_T=x_T)\n",
        "    return samples, intermediates\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "sr_diffMode = 'superresolution'\n",
        "sr_model = get_model('superresolution')\n",
        "\n",
        "\n",
        "\n",
        "output.clear()\n",
        "# !nvidia-smi\n",
        "op(c.ok, 'Setup finished.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "input_img = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <small><font color=\"#999\">Leave empty to save in the same directory where `input_img` resides..</font></small>\n",
        "output_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Latent Diffusion sharpening settings:\n",
        "#@markdown <small><br><font color=\"#999\">The slower the better.</font></small>\n",
        "superres_quality = 'Very Slow' #@param ['Off', 'Faster', 'Fast', 'Slow', 'Very Slow']\n",
        "\n",
        "#@markdown FBCNN JPEG artifact removal settings:\n",
        "#@markdown <small><br><font color=\"#999\">The larger number the cleaner image.</font></small>\n",
        "artifact_removal_rate = 70 #@param {type:\"slider\"}\n",
        "is_gray = False #@param {type:\"boolean\"}\n",
        "\n",
        "input_img = drive_root+input_img\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "img_in = Image.open(input_img)\n",
        "\n",
        "img_state = img_in\n",
        "if superres_quality != 'Off':\n",
        "  img_state = do_superres(img_state, superres_quality)\n",
        "\n",
        "if artifact_removal_rate > 0:\n",
        "  img_state = remove_artifacts(img_state, artifact_removal_rate)\n",
        "\n",
        "img_out = img_state\n",
        "\n",
        "if output_dir is '':\n",
        "  output_dir = path_dir(input_img)\n",
        "\n",
        "img_out_path = output_dir+basename(input_img)+'_superres.png'\n",
        "img_out.save(img_out_path)\n",
        "\n",
        "disp_out = img_out_path.replace(drive_root, '')\n",
        "print('')\n",
        "print('Image saved as', disp_out)\n",
        "op(c.ok, 'FIN.')"
      ],
      "metadata": {
        "id": "TNLr25GAlleC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}